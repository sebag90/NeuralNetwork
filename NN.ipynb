{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from numpy.random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_toy_data(num_samples,num_features, num_classes, seed=3):\n",
    "    # num_samples: number of samples *per class*\n",
    "    # num_features: number of features (excluding bias)\n",
    "    # num_classes: number of class labels\n",
    "    # seed: random seed\n",
    "    np.random.seed(seed)\n",
    "    X=np.zeros((num_samples*num_classes, num_features))\n",
    "    y=np.zeros(num_samples*num_classes)\n",
    "    for c in range(num_classes):\n",
    "        # initialize multivariate normal distribution for this class:\n",
    "        # choose a mean for each feature\n",
    "        means = uniform(low=-10, high=10, size=num_features)\n",
    "        # choose a variance for each feature\n",
    "        var = uniform(low=1.0, high=5, size=num_features)\n",
    "        # for simplicity, all features are uncorrelated (covariance between any two features is 0)\n",
    "        cov = var * np.eye(num_features)\n",
    "        # draw samples from normal distribution\n",
    "        X[c*num_samples:c*num_samples+num_samples,:] = multivariate_normal(means, cov, size=num_samples)\n",
    "        # set label\n",
    "        y[c*num_samples:c*num_samples+num_samples] = c\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    \n",
    "    # initialize network\n",
    "    def __init__(self, sizes, activations):\n",
    "        if len(sizes) -1 != len(activations):\n",
    "            print(\"number of activations must be equal to number of layers - 1\")\n",
    "        else:\n",
    "            self.num_layers = len(sizes)\n",
    "            self.activations = activations\n",
    "            self.sizes = sizes\n",
    "            self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
    "            self.weights = [np.random.randn(y, x) for x, y in zip(sizes[1:], sizes[:-1])]\n",
    "            self.activation_funcs = {\n",
    "                \"relu\" : self.relu,\n",
    "                \"softmax\" : self.softmax,\n",
    "                \"sigmoid\" : self.sigmoid,\n",
    "                \"identity\": self.identity\n",
    "            }\n",
    "     \n",
    "    # activation functions and derivatives\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_prime(self, z):\n",
    "        z[z > 0] = 1\n",
    "        z[z <= 0] = 0\n",
    "        return z\n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0) \n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    def identity(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "    # feed forward\n",
    "    def feedforward(self, a):\n",
    "        for b, w, activation in zip(self.biases, self.weights, self.activations):\n",
    "            activation = self.activation_funcs[activation]\n",
    "            a = activation(np.dot(a, w)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= init_toy_data(2,4,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#res = []\n",
    "#for i in preds:\n",
    "#    res.append(np.where(i == np.amax(i))[0][0])\n",
    "#print(res, y)\n",
    "\n",
    "test_data = []\n",
    "for a, s in zip(X, y):\n",
    "    test_data.append([a, s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01418707, 0.30663032, 0.02499046],\n",
       "       [0.01117907, 0.550413  , 0.01685667],\n",
       "       [0.28230429, 0.00303681, 0.06304545],\n",
       "       [0.36845513, 0.00386532, 0.06154979],\n",
       "       [0.14444801, 0.09434476, 0.52855822],\n",
       "       [0.17942643, 0.0417098 , 0.30499941]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([4, 10, 3], [\"sigmoid\", \"softmax\"])\n",
    "net.feedforward(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
